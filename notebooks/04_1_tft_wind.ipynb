{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 \u2014 Temporal Fusion Transformer: Wind Onshore\n",
    "Point predictions with interpretable attention. 24h ahead, trained 2015\u20132017, tested 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_parquet('../cleaned_data.parquet')\n",
    "df['time'] = pd.to_datetime(df['time'], utc=True)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "try:\n",
    "    t = torch.randn(2, 2, device=device)\n",
    "    _ = t @ t\n",
    "except:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features and normalize using training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'generation wind onshore'\n",
    "tso_col = 'forecast wind onshore day ahead'\n",
    "\n",
    "weather_cols = [\n",
    "    'wind_speed_madrid', 'wind_speed_bilbao', 'wind_speed_barcelona',\n",
    "    'wind_speed_seville', 'wind_speed_valencia',\n",
    "    'pressure_bilbao', 'pressure_barcelona', 'pressure_seville',\n",
    "    'pressure_madrid', 'pressure_valencia',\n",
    "    'humidity_valencia', 'humidity_bilbao',\n",
    "    'temp_barcelona', 'temp_max_barcelona',\n",
    "]\n",
    "time_cols = ['hour', 'month']\n",
    "feature_cols = weather_cols + time_cols + [tso_col]\n",
    "\n",
    "# Normalize using training data stats only\n",
    "train_mask = df['time'].dt.year <= 2017\n",
    "\n",
    "target_mean = df.loc[train_mask, target_col].mean()\n",
    "target_std = df.loc[train_mask, target_col].std()\n",
    "\n",
    "feat_means = df.loc[train_mask, feature_cols].mean()\n",
    "feat_stds = df.loc[train_mask, feature_cols].std().replace(0, 1)\n",
    "\n",
    "target_norm = (df[target_col].values - target_mean) / target_std\n",
    "features_norm = ((df[feature_cols] - feat_means) / feat_stds).fillna(0).values\n",
    "\n",
    "# Combine: [target, features] as input channels\n",
    "all_data = np.column_stack([target_norm, features_norm]).astype(np.float32)\n",
    "\n",
    "print(f\"Input channels: {all_data.shape[1]} (1 target + {len(feature_cols)} features)\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Target mean: {target_mean:.0f} MW, std: {target_std:.0f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliding window dataset \u2014 168h context, 24h prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 168   # 7 days of history\n",
    "prediction_length = 24  # 24h ahead\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, ctx_len, pred_len, start_idx, end_idx):\n",
    "        self.data = data\n",
    "        self.ctx_len = ctx_len\n",
    "        self.pred_len = pred_len\n",
    "        self.start = start_idx\n",
    "        self.end = end_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end - self.start - self.ctx_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.start + idx\n",
    "        x = self.data[i : i + self.ctx_len]                          # (ctx_len, all_channels)\n",
    "        y = self.data[i + self.ctx_len : i + self.ctx_len + self.pred_len, 0]  # (pred_len,)\n",
    "        x_future = self.data[i + self.ctx_len : i + self.ctx_len + self.pred_len, 1:]  # (pred_len, features)\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(x_future),\n",
    "            torch.from_numpy(y),\n",
    "        )\n",
    "\n",
    "train_end = int(train_mask.sum())\n",
    "train_ds = TimeSeriesDataset(all_data, context_length, prediction_length, 0, train_end)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Context: {context_length}h, Prediction: {prediction_length}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFT model \u2014 variable selection, gated residual networks, LSTM, interpretable multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualNetwork(nn.Module):\n",
    "    \"\"\"Core building block: FC -> ELU -> FC -> GLU gate -> LayerNorm + skip connection\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.gate_fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(output_size)\n",
    "        self.skip = nn.Linear(input_size, output_size) if input_size != output_size else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip(x)\n",
    "        h = F.elu(self.fc1(x))\n",
    "        h = self.dropout(h)\n",
    "        output = self.fc2(h)\n",
    "        gate = torch.sigmoid(self.gate_fc(h))\n",
    "        return self.layer_norm(gate * output + residual)\n",
    "\n",
    "\n",
    "class VariableSelectionNetwork(nn.Module):\n",
    "    \"\"\"Learns softmax weights over input variables, applies per-variable GRNs\"\"\"\n",
    "    def __init__(self, num_vars, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_vars = num_vars\n",
    "        self.d_model = d_model\n",
    "        # Per-variable transformation\n",
    "        self.var_transforms = nn.ModuleList([\n",
    "            nn.Linear(1, d_model) for _ in range(num_vars)\n",
    "        ])\n",
    "        # Selection weights from flattened inputs\n",
    "        self.weight_network = GatedResidualNetwork(\n",
    "            num_vars * d_model, d_model, num_vars, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, num_vars)\n",
    "        var_outputs = []\n",
    "        for i in range(self.num_vars):\n",
    "            var_outputs.append(self.var_transforms[i](x[:, :, i:i+1]))\n",
    "\n",
    "        # Stack: (batch, time, num_vars, d_model)\n",
    "        var_stack = torch.stack(var_outputs, dim=2)\n",
    "\n",
    "        # Compute selection weights\n",
    "        flat = var_stack.reshape(x.shape[0], x.shape[1], -1)\n",
    "        weights = F.softmax(self.weight_network(flat), dim=-1)  # (batch, time, num_vars)\n",
    "\n",
    "        # Weighted combination\n",
    "        selected = (var_stack * weights.unsqueeze(-1)).sum(dim=2)\n",
    "        return selected, weights\n",
    "\n",
    "\n",
    "class InterpretableMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with shared value weights for interpretability (Lim et al. 2019)\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, self.d_k)  # Shared across heads\n",
    "        self.out_proj = nn.Linear(self.d_k, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        Q = self.W_q(q).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(k).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(v).unsqueeze(1).expand(-1, self.n_heads, -1, -1)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.mean(dim=1)  # Average over heads\n",
    "        return self.out_proj(context), attn\n",
    "\n",
    "\n",
    "class TemporalFusionTransformer(nn.Module):\n",
    "    \"\"\"TFT: Variable Selection -> LSTM -> Interpretable Attention -> Point Output\"\"\"\n",
    "    def __init__(self, num_observed, num_known_future, d_model=32, n_heads=4,\n",
    "                 n_lstm_layers=1, pred_len=24, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Variable selection for observed (past) and known future inputs\n",
    "        self.obs_vsn = VariableSelectionNetwork(num_observed, d_model, dropout)\n",
    "        self.fut_vsn = VariableSelectionNetwork(num_known_future, d_model, dropout)\n",
    "\n",
    "        # LSTM encoder-decoder for local temporal processing\n",
    "        self.encoder_lstm = nn.LSTM(d_model, d_model, n_lstm_layers,\n",
    "                                     batch_first=True, dropout=dropout if n_lstm_layers > 1 else 0)\n",
    "        self.decoder_lstm = nn.LSTM(d_model, d_model, n_lstm_layers,\n",
    "                                     batch_first=True, dropout=dropout if n_lstm_layers > 1 else 0)\n",
    "\n",
    "        # Post-LSTM gated skip connection\n",
    "        self.lstm_gate = GatedResidualNetwork(d_model, d_model, d_model, dropout)\n",
    "\n",
    "        # Interpretable multi-head attention\n",
    "        self.attention = InterpretableMultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.attn_gate = GatedResidualNetwork(d_model, d_model, d_model, dropout)\n",
    "\n",
    "        # Output\n",
    "        self.output_proj = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x_observed, x_future):\n",
    "        # Variable selection\n",
    "        enc_selected, enc_weights = self.obs_vsn(x_observed)\n",
    "        dec_selected, dec_weights = self.fut_vsn(x_future)\n",
    "\n",
    "        # LSTM encoding\n",
    "        enc_out, (h, c) = self.encoder_lstm(enc_selected)\n",
    "        dec_out, _ = self.decoder_lstm(dec_selected, (h, c))\n",
    "\n",
    "        # Concatenate encoder + decoder outputs\n",
    "        lstm_out = torch.cat([enc_out, dec_out], dim=1)\n",
    "        input_cat = torch.cat([enc_selected, dec_selected], dim=1)\n",
    "\n",
    "        # Gated skip connection\n",
    "        lstm_out = self.lstm_gate(lstm_out) + input_cat\n",
    "\n",
    "        # Self-attention over full sequence\n",
    "        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "\n",
    "        # Gated skip connection after attention\n",
    "        attn_out = self.attn_gate(attn_out) + lstm_out\n",
    "\n",
    "        # Extract decoder positions and project to output\n",
    "        decoder_out = attn_out[:, -self.pred_len:, :]\n",
    "        output = self.output_proj(decoder_out).squeeze(-1)\n",
    "\n",
    "        return output, enc_weights, attn_weights\n",
    "\n",
    "\n",
    "num_observed = all_data.shape[1]     # target + all features\n",
    "num_known_future = len(feature_cols)  # features only (no target in future)\n",
    "\n",
    "model = TemporalFusionTransformer(\n",
    "    num_observed=num_observed,\n",
    "    num_known_future=num_known_future,\n",
    "    d_model=32,\n",
    "    n_heads=4,\n",
    "    n_lstm_layers=1,\n",
    "    pred_len=prediction_length,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "print(f\"TFT parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with MSE loss, cosine annealing LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "n_epochs = 30\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for x_ctx, x_fut, y in train_loader:\n",
    "        x_ctx, x_fut, y = x_ctx.to(device), x_fut.to(device), y.to(device)\n",
    "        preds, _, _ = model(x_ctx, x_fut)\n",
    "        loss = criterion(preds, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = np.mean(losses)\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 24h-ahead forecasts on 2018 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_start = train_end\n",
    "test_end = len(all_data)\n",
    "\n",
    "all_preds = []\n",
    "all_actuals = []\n",
    "all_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_start, test_end - prediction_length, prediction_length):\n",
    "        if i - context_length < 0:\n",
    "            continue\n",
    "\n",
    "        x_ctx = torch.from_numpy(all_data[i - context_length : i]).unsqueeze(0).to(device)\n",
    "        x_fut = torch.from_numpy(all_data[i : i + prediction_length, 1:]).unsqueeze(0).to(device)\n",
    "\n",
    "        preds, _, _ = model(x_ctx, x_fut)\n",
    "\n",
    "        # Denormalize\n",
    "        pred_mw = preds.squeeze().cpu().numpy() * target_std + target_mean\n",
    "        actual_mw = all_data[i : i + prediction_length, 0] * target_std + target_mean\n",
    "        times = df['time'].iloc[i : i + prediction_length].values\n",
    "\n",
    "        all_preds.append(pred_mw)\n",
    "        all_actuals.append(actual_mw)\n",
    "        all_times.append(times)\n",
    "\n",
    "print(f\"Generated {len(all_preds)} forecast windows across 2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: MAE, RMSE, MAPE vs TSO baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all windows for aggregate metrics\n",
    "flat_preds = np.concatenate(all_preds)\n",
    "flat_actuals = np.concatenate(all_actuals)\n",
    "\n",
    "# TSO baseline\n",
    "flat_tso = []\n",
    "for w in range(len(all_preds)):\n",
    "    idx = test_start + w * prediction_length\n",
    "    tso_vals = df[tso_col].iloc[idx : idx + prediction_length].values\n",
    "    flat_tso.append(tso_vals)\n",
    "flat_tso = np.concatenate(flat_tso)\n",
    "\n",
    "# TFT metrics\n",
    "tft_mae = np.mean(np.abs(flat_actuals - flat_preds))\n",
    "tft_rmse = np.sqrt(np.mean((flat_actuals - flat_preds) ** 2))\n",
    "tft_mape = np.mean(np.abs((flat_actuals - flat_preds) / np.clip(flat_actuals, 1, None))) * 100\n",
    "\n",
    "# TSO metrics\n",
    "tso_mae_val = np.mean(np.abs(flat_actuals - flat_tso))\n",
    "tso_rmse_val = np.sqrt(np.mean((flat_actuals - flat_tso) ** 2))\n",
    "tso_mape_val = np.mean(np.abs((flat_actuals - flat_tso) / np.clip(flat_actuals, 1, None))) * 100\n",
    "\n",
    "print(f\"{'Metric':<10} {'TFT':>10} {'TSO':>10} {'Improvement':>12}\")\n",
    "print('-' * 44)\n",
    "print(f\"{'MAE (MW)':<10} {tft_mae:>10.1f} {tso_mae_val:>10.1f} {(1 - tft_mae / tso_mae_val) * 100:>+11.1f}%\")\n",
    "print(f\"{'RMSE (MW)':<10} {tft_rmse:>10.1f} {tso_rmse_val:>10.1f} {(1 - tft_rmse / tso_rmse_val) * 100:>+11.1f}%\")\n",
    "print(f\"{'MAPE (%)':<10} {tft_mape:>10.1f} {tso_mape_val:>10.1f} {(1 - tft_mape / tso_mape_val) * 100:>+11.1f}%\")\n",
    "\n",
    "# Per-hour MAE across the 24h forecast horizon\n",
    "per_hour_mae = np.zeros(prediction_length)\n",
    "for h in range(prediction_length):\n",
    "    h_preds = np.array([p[h] for p in all_preds])\n",
    "    h_actuals = np.array([a[h] for a in all_actuals])\n",
    "    per_hour_mae[h] = np.mean(np.abs(h_actuals - h_preds))\n",
    "\n",
    "print(f\"\\nPer-hour MAE (MW):\")\n",
    "print(f\"  h+1:  {per_hour_mae[0]:.0f}  |  h+6:  {per_hour_mae[5]:.0f}  |  h+12: {per_hour_mae[11]:.0f}  |  h+24: {per_hour_mae[23]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted vs actual \u2014 sample week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_windows = range(9, 16)\n",
    "\n",
    "sample_pred = []\n",
    "sample_actual = []\n",
    "sample_tso = []\n",
    "sample_time = []\n",
    "\n",
    "for w in sample_windows:\n",
    "    if w >= len(all_preds):\n",
    "        break\n",
    "    sample_pred.extend(all_preds[w])\n",
    "    sample_actual.extend(all_actuals[w])\n",
    "    sample_time.extend(pd.to_datetime(all_times[w]))\n",
    "    idx = test_start + w * prediction_length\n",
    "    sample_tso.extend(df[tso_col].iloc[idx : idx + prediction_length].values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(sample_time, sample_actual, color='#2c3e50', linewidth=1.5, label='Actual')\n",
    "ax.plot(sample_time, sample_pred, color='coral', linewidth=1.3, label='TFT Predicted')\n",
    "ax.plot(sample_time, sample_tso, color='grey', linewidth=1.0, linestyle='--', alpha=0.7, label='TSO Forecast')\n",
    "ax.set_ylabel('MW')\n",
    "ax.set_title('TFT Wind Onshore \\u2014 Predicted vs Actual (Sample Week, Jan 2018)')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export JSON for dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../dashboard/public/data', exist_ok=True)\n",
    "\n",
    "# Build sample week data\n",
    "sample_data = []\n",
    "for w in range(9, 16):\n",
    "    if w >= len(all_preds):\n",
    "        break\n",
    "    pred = all_preds[w]\n",
    "    actual = all_actuals[w]\n",
    "    times = all_times[w]\n",
    "    tso = df[tso_col].iloc[test_start + w * prediction_length : test_start + w * prediction_length + prediction_length].values\n",
    "    for h in range(prediction_length):\n",
    "        t = pd.Timestamp(times[h])\n",
    "        sample_data.append({\n",
    "            'time': t.strftime('%Y-%m-%d %H:%M'),\n",
    "            'actual': round(float(actual[h]), 1),\n",
    "            'predicted': round(float(pred[h]), 1),\n",
    "            'tso': round(float(tso[h]), 1),\n",
    "        })\n",
    "\n",
    "output = {\n",
    "    'target': 'wind_onshore',\n",
    "    'model': 'Temporal Fusion Transformer',\n",
    "    'prediction_length_hours': prediction_length,\n",
    "    'context_length_hours': context_length,\n",
    "    'metrics': {\n",
    "        'mae': round(float(tft_mae), 1),\n",
    "        'rmse': round(float(tft_rmse), 1),\n",
    "        'mape': round(float(tft_mape), 1),\n",
    "        'tso_mae': round(float(tso_mae_val), 1),\n",
    "        'tso_rmse': round(float(tso_rmse_val), 1),\n",
    "    },\n",
    "    'sample_forecast': sample_data,\n",
    "}\n",
    "\n",
    "with open('../dashboard/public/data/tft_wind.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print('Saved tft_wind.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}