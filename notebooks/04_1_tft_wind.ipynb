{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 — Temporal Fusion Transformer: Wind Onshore\n",
    "Point predictions with interpretable attention. 24h ahead, trained 2015–2017, tested 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:01:40.939001Z",
     "iopub.status.busy": "2026-02-17T15:01:40.938457Z",
     "iopub.status.idle": "2026-02-17T15:01:41.776467Z",
     "shell.execute_reply": "2026-02-17T15:01:41.776196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (35056, 80)\n",
      "PyTorch: 2.10.0\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_parquet('../cleaned_data.parquet')\n",
    "df['time'] = pd.to_datetime(df['time'], utc=True)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "try:\n",
    "    t = torch.randn(2, 2, device=device)\n",
    "    _ = t @ t\n",
    "except:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features and normalize using training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:01:41.791033Z",
     "iopub.status.busy": "2026-02-17T15:01:41.790871Z",
     "iopub.status.idle": "2026-02-17T15:01:41.802045Z",
     "shell.execute_reply": "2026-02-17T15:01:41.801810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input channels: 18 (1 target + 17 features)\n",
      "Features: ['wind_speed_madrid', 'wind_speed_bilbao', 'wind_speed_barcelona', 'wind_speed_seville', 'wind_speed_valencia', 'pressure_bilbao', 'pressure_barcelona', 'pressure_seville', 'pressure_madrid', 'pressure_valencia', 'humidity_valencia', 'humidity_bilbao', 'temp_barcelona', 'temp_max_barcelona', 'hour', 'month', 'forecast wind onshore day ahead']\n",
      "Target mean: 5426 MW, std: 3181 MW\n"
     ]
    }
   ],
   "source": [
    "target_col = 'generation wind onshore'\n",
    "tso_col = 'forecast wind onshore day ahead'\n",
    "\n",
    "weather_cols = [\n",
    "    'wind_speed_madrid', 'wind_speed_bilbao', 'wind_speed_barcelona',\n",
    "    'wind_speed_seville', 'wind_speed_valencia',\n",
    "    'pressure_bilbao', 'pressure_barcelona', 'pressure_seville',\n",
    "    'pressure_madrid', 'pressure_valencia',\n",
    "    'humidity_valencia', 'humidity_bilbao',\n",
    "    'temp_barcelona', 'temp_max_barcelona',\n",
    "]\n",
    "time_cols = ['hour', 'month']\n",
    "feature_cols = weather_cols + time_cols + [tso_col]\n",
    "\n",
    "# Normalize using training data stats only\n",
    "train_mask = df['time'].dt.year <= 2017\n",
    "\n",
    "target_mean = df.loc[train_mask, target_col].mean()\n",
    "target_std = df.loc[train_mask, target_col].std()\n",
    "\n",
    "feat_means = df.loc[train_mask, feature_cols].mean()\n",
    "feat_stds = df.loc[train_mask, feature_cols].std().replace(0, 1)\n",
    "\n",
    "target_norm = (df[target_col].values - target_mean) / target_std\n",
    "features_norm = ((df[feature_cols] - feat_means) / feat_stds).fillna(0).values\n",
    "\n",
    "# Combine: [target, features] as input channels\n",
    "all_data = np.column_stack([target_norm, features_norm]).astype(np.float32)\n",
    "\n",
    "print(f\"Input channels: {all_data.shape[1]} (1 target + {len(feature_cols)} features)\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Target mean: {target_mean:.0f} MW, std: {target_std:.0f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliding window dataset — 168h context, 24h prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:01:41.803175Z",
     "iopub.status.busy": "2026-02-17T15:01:41.803100Z",
     "iopub.status.idle": "2026-02-17T15:01:41.805930Z",
     "shell.execute_reply": "2026-02-17T15:01:41.805726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 20846, Val samples: 5069\n",
      "Context: 168h, Prediction: 24h\n"
     ]
    }
   ],
   "source": [
    "context_length = 168   # 7 days of history\n",
    "prediction_length = 24  # 24h ahead\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, ctx_len, pred_len, start_idx, end_idx):\n",
    "        self.data = data\n",
    "        self.ctx_len = ctx_len\n",
    "        self.pred_len = pred_len\n",
    "        self.start = start_idx\n",
    "        self.end = end_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end - self.start - self.ctx_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.start + idx\n",
    "        x = self.data[i : i + self.ctx_len]                          # (ctx_len, all_channels)\n",
    "        y = self.data[i + self.ctx_len : i + self.ctx_len + self.pred_len, 0]  # (pred_len,)\n",
    "        x_future = self.data[i + self.ctx_len : i + self.ctx_len + self.pred_len, 1:]  # (pred_len, features)\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(x_future),\n",
    "            torch.from_numpy(y),\n",
    "        )\n",
    "\n",
    "train_end = int(train_mask.sum())\n",
    "val_split = int(train_end * 0.8)\n",
    "\n",
    "train_ds = TimeSeriesDataset(all_data, context_length, prediction_length, 0, val_split)\n",
    "val_ds = TimeSeriesDataset(all_data, context_length, prediction_length, val_split, train_end)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n",
    "print(f\"Context: {context_length}h, Prediction: {prediction_length}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFT model — variable selection, gated residual networks, LSTM, interpretable multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:01:41.806989Z",
     "iopub.status.busy": "2026-02-17T15:01:41.806908Z",
     "iopub.status.idle": "2026-02-17T15:01:41.852942Z",
     "shell.execute_reply": "2026-02-17T15:01:41.852731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFT parameters: 27,972\n"
     ]
    }
   ],
   "source": [
    "class GatedResidualNetwork(nn.Module):\n",
    "    \"\"\"Core building block: FC -> ELU -> FC -> GLU gate -> LayerNorm + skip connection\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.gate_fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(output_size)\n",
    "        self.skip = nn.Linear(input_size, output_size) if input_size != output_size else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip(x)\n",
    "        h = F.elu(self.fc1(x))\n",
    "        h = self.dropout(h)\n",
    "        output = self.fc2(h)\n",
    "        gate = torch.sigmoid(self.gate_fc(h))\n",
    "        return self.layer_norm(gate * output + residual)\n",
    "\n",
    "\n",
    "class VariableSelectionNetwork(nn.Module):\n",
    "    \"\"\"Learns softmax weights over input variables, applies per-variable GRNs\"\"\"\n",
    "    def __init__(self, num_vars, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_vars = num_vars\n",
    "        self.d_model = d_model\n",
    "        # Per-variable transformation\n",
    "        self.var_transforms = nn.ModuleList([\n",
    "            nn.Linear(1, d_model) for _ in range(num_vars)\n",
    "        ])\n",
    "        # Selection weights from flattened inputs\n",
    "        self.weight_network = GatedResidualNetwork(\n",
    "            num_vars * d_model, d_model, num_vars, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, num_vars)\n",
    "        var_outputs = []\n",
    "        for i in range(self.num_vars):\n",
    "            var_outputs.append(self.var_transforms[i](x[:, :, i:i+1]))\n",
    "\n",
    "        # Stack: (batch, time, num_vars, d_model)\n",
    "        var_stack = torch.stack(var_outputs, dim=2)\n",
    "\n",
    "        # Compute selection weights\n",
    "        flat = var_stack.reshape(x.shape[0], x.shape[1], -1)\n",
    "        weights = F.softmax(self.weight_network(flat), dim=-1)  # (batch, time, num_vars)\n",
    "\n",
    "        # Weighted combination\n",
    "        selected = (var_stack * weights.unsqueeze(-1)).sum(dim=2)\n",
    "        return selected, weights\n",
    "\n",
    "\n",
    "class InterpretableMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with shared value weights for interpretability (Lim et al. 2019)\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, self.d_k)  # Shared across heads\n",
    "        self.out_proj = nn.Linear(self.d_k, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        Q = self.W_q(q).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(k).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(v).unsqueeze(1).expand(-1, self.n_heads, -1, -1)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.mean(dim=1)  # Average over heads\n",
    "        return self.out_proj(context), attn\n",
    "\n",
    "\n",
    "class TemporalFusionTransformer(nn.Module):\n",
    "    \"\"\"TFT: Variable Selection -> LSTM -> Interpretable Attention -> Point Output\"\"\"\n",
    "    def __init__(self, num_observed, num_known_future, d_model=32, n_heads=4,\n",
    "                 n_lstm_layers=1, pred_len=24, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Variable selection for observed (past) and known future inputs\n",
    "        self.obs_vsn = VariableSelectionNetwork(num_observed, d_model, dropout)\n",
    "        self.fut_vsn = VariableSelectionNetwork(num_known_future, d_model, dropout)\n",
    "\n",
    "        # LSTM encoder-decoder for local temporal processing\n",
    "        self.encoder_lstm = nn.LSTM(d_model, d_model, n_lstm_layers,\n",
    "                                     batch_first=True, dropout=dropout if n_lstm_layers > 1 else 0)\n",
    "        self.decoder_lstm = nn.LSTM(d_model, d_model, n_lstm_layers,\n",
    "                                     batch_first=True, dropout=dropout if n_lstm_layers > 1 else 0)\n",
    "\n",
    "        # Post-LSTM gated skip connection\n",
    "        self.lstm_gate = GatedResidualNetwork(d_model, d_model, d_model, dropout)\n",
    "\n",
    "        # Interpretable multi-head attention\n",
    "        self.attention = InterpretableMultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.attn_gate = GatedResidualNetwork(d_model, d_model, d_model, dropout)\n",
    "\n",
    "        # Output\n",
    "        self.output_proj = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x_observed, x_future):\n",
    "        # Variable selection\n",
    "        enc_selected, enc_weights = self.obs_vsn(x_observed)\n",
    "        dec_selected, dec_weights = self.fut_vsn(x_future)\n",
    "\n",
    "        # LSTM encoding\n",
    "        enc_out, (h, c) = self.encoder_lstm(enc_selected)\n",
    "        dec_out, _ = self.decoder_lstm(dec_selected, (h, c))\n",
    "\n",
    "        # Concatenate encoder + decoder outputs\n",
    "        lstm_out = torch.cat([enc_out, dec_out], dim=1)\n",
    "        input_cat = torch.cat([enc_selected, dec_selected], dim=1)\n",
    "\n",
    "        # Gated skip connection\n",
    "        lstm_out = self.lstm_gate(lstm_out) + input_cat\n",
    "\n",
    "        # Self-attention over full sequence\n",
    "        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "\n",
    "        # Gated skip connection after attention\n",
    "        attn_out = self.attn_gate(attn_out) + lstm_out\n",
    "\n",
    "        # Extract decoder positions and project to output\n",
    "        decoder_out = attn_out[:, -self.pred_len:, :]\n",
    "        output = self.output_proj(decoder_out).squeeze(-1)\n",
    "\n",
    "        return output, enc_weights, attn_weights\n",
    "\n",
    "\n",
    "num_observed = all_data.shape[1]     # target + all features\n",
    "num_known_future = len(feature_cols)  # features only (no target in future)\n",
    "\n",
    "model = TemporalFusionTransformer(\n",
    "    num_observed=num_observed,\n",
    "    num_known_future=num_known_future,\n",
    "    d_model=16,\n",
    "    n_heads=4,\n",
    "    n_lstm_layers=1,\n",
    "    pred_len=prediction_length,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "print(f\"TFT parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with MSE loss, early stopping (patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:01:41.854020Z",
     "iopub.status.busy": "2026-02-17T15:01:41.853947Z",
     "iopub.status.idle": "2026-02-17T15:14:44.898068Z",
     "shell.execute_reply": "2026-02-17T15:14:44.897363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100, Train: 0.08338, Val: 0.00621, LR: 0.001000 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/100, Train: 0.00309, Val: 0.00525, LR: 0.000999 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/100, Train: 0.00241, Val: 0.00502, LR: 0.000998 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/100, Train: 0.00222, Val: 0.00494, LR: 0.000996 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/100, Train: 0.00212, Val: 0.00493, LR: 0.000994 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/100, Train: 0.00207, Val: 0.00493, LR: 0.000991 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7/100, Train: 0.00203, Val: 0.00484, LR: 0.000988 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/100, Train: 0.00200, Val: 0.00482, LR: 0.000984 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9/100, Train: 0.00198, Val: 0.00482, LR: 0.000980 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/100, Train: 0.00197, Val: 0.00480, LR: 0.000976 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12/100, Train: 0.00195, Val: 0.00479, LR: 0.000965 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14/100, Train: 0.00194, Val: 0.00478, LR: 0.000952 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16/100, Train: 0.00191, Val: 0.00476, LR: 0.000938 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17/100, Train: 0.00191, Val: 0.00475, LR: 0.000930 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/100, Train: 0.00189, Val: 0.00476, LR: 0.000905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  21/100, Train: 0.00188, Val: 0.00472, LR: 0.000895 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23/100, Train: 0.00188, Val: 0.00471, LR: 0.000875 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  29/100, Train: 0.00185, Val: 0.00470, LR: 0.000806 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/100, Train: 0.00185, Val: 0.00473, LR: 0.000794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  36/100, Train: 0.00182, Val: 0.00470, LR: 0.000713 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  39/100, Train: 0.00182, Val: 0.00469, LR: 0.000669 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/100, Train: 0.00180, Val: 0.00470, LR: 0.000655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  43/100, Train: 0.00180, Val: 0.00468, LR: 0.000609 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  47/100, Train: 0.00177, Val: 0.00468, LR: 0.000547 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50/100, Train: 0.00175, Val: 0.00466, LR: 0.000500 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  54/100, Train: 0.00175, Val: 0.00466, LR: 0.000437 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  56/100, Train: 0.00173, Val: 0.00465, LR: 0.000406 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60/100, Train: 0.00171, Val: 0.00472, LR: 0.000345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  70/100, Train: 0.00167, Val: 0.00472, LR: 0.000206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 71 (no improvement for 15 epochs)\n",
      "Restored best model (val loss: 0.00465)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 15\n",
    "best_val_loss = float('inf')\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x_ctx, x_fut, y in train_loader:\n",
    "        x_ctx, x_fut, y = x_ctx.to(device), x_fut.to(device), y.to(device)\n",
    "        preds, _, _ = model(x_ctx, x_fut)\n",
    "        loss = criterion(preds, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_ctx, x_fut, y in val_loader:\n",
    "            x_ctx, x_fut, y = x_ctx.to(device), x_fut.to(device), y.to(device)\n",
    "            preds, _, _ = model(x_ctx, x_fut)\n",
    "            val_losses.append(criterion(preds, y).item())\n",
    "    \n",
    "    scheduler.step()\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "        marker = ' *'\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        marker = ''\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0 or epochs_no_improve == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{n_epochs}, Train: {train_loss:.5f}, Val: {val_loss:.5f}, LR: {scheduler.get_last_lr()[0]:.6f}{marker}\")\n",
    "    \n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "        break\n",
    "\n",
    "# Restore best weights\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"Restored best model (val loss: {best_val_loss:.5f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 24h-ahead forecasts on 2018 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:14:44.901267Z",
     "iopub.status.busy": "2026-02-17T15:14:44.900621Z",
     "iopub.status.idle": "2026-02-17T15:14:47.810510Z",
     "shell.execute_reply": "2026-02-17T15:14:47.810194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 364 forecast windows across 2018\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_start = train_end\n",
    "test_end = len(all_data)\n",
    "\n",
    "all_preds = []\n",
    "all_actuals = []\n",
    "all_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_start, test_end - prediction_length, prediction_length):\n",
    "        if i - context_length < 0:\n",
    "            continue\n",
    "\n",
    "        x_ctx = torch.from_numpy(all_data[i - context_length : i]).unsqueeze(0).to(device)\n",
    "        x_fut = torch.from_numpy(all_data[i : i + prediction_length, 1:]).unsqueeze(0).to(device)\n",
    "\n",
    "        preds, _, _ = model(x_ctx, x_fut)\n",
    "\n",
    "        # Denormalize\n",
    "        pred_mw = preds.squeeze().cpu().numpy() * target_std + target_mean\n",
    "        actual_mw = all_data[i : i + prediction_length, 0] * target_std + target_mean\n",
    "        times = df['time'].iloc[i : i + prediction_length].values\n",
    "\n",
    "        all_preds.append(pred_mw)\n",
    "        all_actuals.append(actual_mw)\n",
    "        all_times.append(times)\n",
    "\n",
    "print(f\"Generated {len(all_preds)} forecast windows across 2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: MAE, RMSE, MAPE vs TSO baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:14:47.812121Z",
     "iopub.status.busy": "2026-02-17T15:14:47.812026Z",
     "iopub.status.idle": "2026-02-17T15:14:47.819447Z",
     "shell.execute_reply": "2026-02-17T15:14:47.819217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric            TFT        TSO  Improvement\n",
      "--------------------------------------------\n",
      "MAE (MW)        448.0      448.1        +0.0%\n",
      "RMSE (MW)       615.3      614.0        -0.2%\n",
      "MAPE (%)         11.6       11.6        +0.1%\n",
      "\n",
      "Per-hour MAE (MW):\n",
      "  h+1:  368  |  h+6:  400  |  h+12: 459  |  h+24: 386\n"
     ]
    }
   ],
   "source": [
    "# Flatten all windows for aggregate metrics\n",
    "flat_preds = np.concatenate(all_preds)\n",
    "flat_actuals = np.concatenate(all_actuals)\n",
    "\n",
    "# TSO baseline\n",
    "flat_tso = []\n",
    "for w in range(len(all_preds)):\n",
    "    idx = test_start + w * prediction_length\n",
    "    tso_vals = df[tso_col].iloc[idx : idx + prediction_length].values\n",
    "    flat_tso.append(tso_vals)\n",
    "flat_tso = np.concatenate(flat_tso)\n",
    "\n",
    "# TFT metrics\n",
    "tft_mae = np.mean(np.abs(flat_actuals - flat_preds))\n",
    "tft_rmse = np.sqrt(np.mean((flat_actuals - flat_preds) ** 2))\n",
    "tft_mape = np.mean(np.abs((flat_actuals - flat_preds) / np.clip(flat_actuals, 1, None))) * 100\n",
    "\n",
    "# TSO metrics\n",
    "tso_mae_val = np.mean(np.abs(flat_actuals - flat_tso))\n",
    "tso_rmse_val = np.sqrt(np.mean((flat_actuals - flat_tso) ** 2))\n",
    "tso_mape_val = np.mean(np.abs((flat_actuals - flat_tso) / np.clip(flat_actuals, 1, None))) * 100\n",
    "\n",
    "print(f\"{'Metric':<10} {'TFT':>10} {'TSO':>10} {'Improvement':>12}\")\n",
    "print('-' * 44)\n",
    "print(f\"{'MAE (MW)':<10} {tft_mae:>10.1f} {tso_mae_val:>10.1f} {(1 - tft_mae / tso_mae_val) * 100:>+11.1f}%\")\n",
    "print(f\"{'RMSE (MW)':<10} {tft_rmse:>10.1f} {tso_rmse_val:>10.1f} {(1 - tft_rmse / tso_rmse_val) * 100:>+11.1f}%\")\n",
    "print(f\"{'MAPE (%)':<10} {tft_mape:>10.1f} {tso_mape_val:>10.1f} {(1 - tft_mape / tso_mape_val) * 100:>+11.1f}%\")\n",
    "\n",
    "# Per-hour MAE across the 24h forecast horizon\n",
    "per_hour_mae = np.zeros(prediction_length)\n",
    "for h in range(prediction_length):\n",
    "    h_preds = np.array([p[h] for p in all_preds])\n",
    "    h_actuals = np.array([a[h] for a in all_actuals])\n",
    "    per_hour_mae[h] = np.mean(np.abs(h_actuals - h_preds))\n",
    "\n",
    "print(f\"\\nPer-hour MAE (MW):\")\n",
    "print(f\"  h+1:  {per_hour_mae[0]:.0f}  |  h+6:  {per_hour_mae[5]:.0f}  |  h+12: {per_hour_mae[11]:.0f}  |  h+24: {per_hour_mae[23]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "XGBoost residual correction — per-horizon models trained on validation-set TFT errors",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "sample_windows = range(9, 16)\n\nsample_corrected = []\nsample_actual = []\nsample_tso = []\nsample_time = []\n\nfor w in sample_windows:\n    if w >= len(all_preds):\n        break\n    sample_corrected.extend(corrected_preds[w])\n    sample_actual.extend(all_actuals[w])\n    sample_time.extend(pd.to_datetime(all_times[w]))\n    idx = test_start + w * prediction_length\n    sample_tso.extend(df[tso_col].iloc[idx : idx + prediction_length].values)\n\nfig, ax = plt.subplots(figsize=(14, 5))\nax.plot(sample_time, sample_actual, color='#2c3e50', linewidth=1.5, label='Actual')\nax.plot(sample_time, sample_corrected, color='coral', linewidth=1.3, label='TFT + XGBoost')\nax.plot(sample_time, sample_tso, color='grey', linewidth=1.0, linestyle='--', alpha=0.7, label='TSO Forecast')\nax.set_ylabel('MW')\nax.set_title('TFT Wind Onshore — Predicted vs Actual (Sample Week, Jan 2018)')\nax.legend()\nplt.xticks(rotation=30)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted vs actual — sample week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:14:47.820486Z",
     "iopub.status.busy": "2026-02-17T15:14:47.820411Z",
     "iopub.status.idle": "2026-02-17T15:14:47.923340Z",
     "shell.execute_reply": "2026-02-17T15:14:47.923106Z"
    }
   },
   "outputs": [],
   "source": "import os\nos.makedirs('../dashboard/public/data', exist_ok=True)\n\n# Build sample week data using corrected predictions\nsample_data = []\nfor w in range(9, 16):\n    if w >= len(all_preds):\n        break\n    pred = corrected_preds[w]\n    actual = all_actuals[w]\n    times = all_times[w]\n    tso = df[tso_col].iloc[test_start + w * prediction_length : test_start + w * prediction_length + prediction_length].values\n    for h in range(prediction_length):\n        t = pd.Timestamp(times[h])\n        sample_data.append({\n            'time': t.strftime('%Y-%m-%d %H:%M'),\n            'actual': round(float(actual[h]), 1),\n            'predicted': round(float(pred[h]), 1),\n            'tso': round(float(tso[h]), 1),\n        })\n\noutput = {\n    'target': 'wind_onshore',\n    'model': 'TFT + XGBoost Residual Correction',\n    'prediction_length_hours': prediction_length,\n    'context_length_hours': context_length,\n    'metrics': {\n        'mae': round(float(corr_mae), 1),\n        'rmse': round(float(corr_rmse), 1),\n        'mape': round(float(corr_mape), 1),\n        'tso_mae': round(float(tso_mae_val), 1),\n        'tso_rmse': round(float(tso_rmse_val), 1),\n        'raw_mae': round(float(tft_mae), 1),\n        'raw_rmse': round(float(tft_rmse), 1),\n    },\n    'sample_forecast': sample_data,\n}\n\nwith open('../dashboard/public/data/tft_wind.json', 'w') as f:\n    json.dump(output, f, indent=2)\n\nprint('Saved tft_wind.json')\nprint(f\"Raw  MAE: {tft_mae:.1f} MW → Corrected MAE: {corr_mae:.1f} MW (TSO: {tso_mae_val:.1f} MW)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export JSON for dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:14:47.924414Z",
     "iopub.status.busy": "2026-02-17T15:14:47.924335Z",
     "iopub.status.idle": "2026-02-17T15:14:47.928400Z",
     "shell.execute_reply": "2026-02-17T15:14:47.928217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tft_wind.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('../dashboard/public/data', exist_ok=True)\n",
    "\n",
    "# Build sample week data\n",
    "sample_data = []\n",
    "for w in range(9, 16):\n",
    "    if w >= len(all_preds):\n",
    "        break\n",
    "    pred = all_preds[w]\n",
    "    actual = all_actuals[w]\n",
    "    times = all_times[w]\n",
    "    tso = df[tso_col].iloc[test_start + w * prediction_length : test_start + w * prediction_length + prediction_length].values\n",
    "    for h in range(prediction_length):\n",
    "        t = pd.Timestamp(times[h])\n",
    "        sample_data.append({\n",
    "            'time': t.strftime('%Y-%m-%d %H:%M'),\n",
    "            'actual': round(float(actual[h]), 1),\n",
    "            'predicted': round(float(pred[h]), 1),\n",
    "            'tso': round(float(tso[h]), 1),\n",
    "        })\n",
    "\n",
    "output = {\n",
    "    'target': 'wind_onshore',\n",
    "    'model': 'Temporal Fusion Transformer',\n",
    "    'prediction_length_hours': prediction_length,\n",
    "    'context_length_hours': context_length,\n",
    "    'metrics': {\n",
    "        'mae': round(float(tft_mae), 1),\n",
    "        'rmse': round(float(tft_rmse), 1),\n",
    "        'mape': round(float(tft_mape), 1),\n",
    "        'tso_mae': round(float(tso_mae_val), 1),\n",
    "        'tso_rmse': round(float(tso_rmse_val), 1),\n",
    "    },\n",
    "    'sample_forecast': sample_data,\n",
    "}\n",
    "\n",
    "with open('../dashboard/public/data/tft_wind.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print('Saved tft_wind.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}