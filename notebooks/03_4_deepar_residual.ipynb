{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 3.4 — DeepAR-style Probabilistic Forecast: Residual Load\n",
    "LSTM with Gaussian output for confidence intervals. 24h ahead, trained on 2015–2017, tested on 2018.\n",
    "\n",
    "Residual load = total load - wind onshore - solar. No TSO forecast available for this derived target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:43:57.151707Z",
     "iopub.status.busy": "2026-02-16T16:43:57.151537Z",
     "iopub.status.idle": "2026-02-16T16:43:58.166801Z",
     "shell.execute_reply": "2026-02-16T16:43:58.166583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (35056, 81)\n",
      "PyTorch: 2.10.0\n",
      "Device: mps\n",
      "Residual load mean: 21800 MW\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_parquet('../cleaned_data.parquet')\n",
    "df['time'] = pd.to_datetime(df['time'], utc=True)\n",
    "\n",
    "# Compute residual load\n",
    "df['residual_load'] = df['total load actual'] - df['generation wind onshore'] - df['generation solar']\n",
    "\n",
    "# Use MPS (Apple Silicon GPU) if available\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "try:\n",
    "    t = torch.randn(2, 2, device=device)\n",
    "    _ = t @ t\n",
    "except:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Residual load mean: {df['residual_load'].mean():.0f} MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:43:58.167894Z",
     "iopub.status.busy": "2026-02-16T16:43:58.167784Z",
     "iopub.status.idle": "2026-02-16T16:43:58.182306Z",
     "shell.execute_reply": "2026-02-16T16:43:58.182084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input channels: 23 (1 target + 22 features)\n",
      "Target mean: 21698 MW, std: 5022 MW\n"
     ]
    }
   ],
   "source": [
    "# Prepare features: weather + time (broad set — residual depends on wind + solar + demand)\n",
    "target_col = 'residual_load'\n",
    "weather_cols = [\n",
    "    'wind_speed_madrid', 'wind_speed_bilbao', 'wind_speed_barcelona',\n",
    "    'wind_speed_seville', 'wind_speed_valencia',\n",
    "    'pressure_madrid', 'pressure_bilbao', 'pressure_barcelona',\n",
    "    'pressure_seville', 'pressure_valencia',\n",
    "    'clouds_all_madrid', 'clouds_all_bilbao', 'clouds_all_barcelona',\n",
    "    'clouds_all_seville', 'clouds_all_valencia',\n",
    "    'temp_madrid', 'temp_bilbao', 'temp_barcelona',\n",
    "    'temp_seville', 'temp_valencia',\n",
    "]\n",
    "time_cols = ['hour', 'month']\n",
    "feature_cols = weather_cols + time_cols\n",
    "\n",
    "# Normalize features and target\n",
    "train_mask = df['time'].dt.year <= 2017\n",
    "\n",
    "# Compute stats on training data only\n",
    "target_mean = df.loc[train_mask, target_col].mean()\n",
    "target_std = df.loc[train_mask, target_col].std()\n",
    "\n",
    "feat_means = df.loc[train_mask, feature_cols].mean()\n",
    "feat_stds = df.loc[train_mask, feature_cols].std().replace(0, 1)\n",
    "\n",
    "# Normalize\n",
    "target_norm = (df[target_col].values - target_mean) / target_std\n",
    "features_norm = ((df[feature_cols] - feat_means) / feat_stds).fillna(0).values\n",
    "\n",
    "# Combine: [target, features] as input channels\n",
    "all_data = np.column_stack([target_norm, features_norm]).astype(np.float32)\n",
    "\n",
    "print(f\"Input channels: {all_data.shape[1]} (1 target + {len(feature_cols)} features)\")\n",
    "print(f\"Target mean: {target_mean:.0f} MW, std: {target_std:.0f} MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:43:58.183326Z",
     "iopub.status.busy": "2026-02-16T16:43:58.183256Z",
     "iopub.status.idle": "2026-02-16T16:43:58.185925Z",
     "shell.execute_reply": "2026-02-16T16:43:58.185755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 26106\n",
      "Context: 168h, Prediction: 24h\n"
     ]
    }
   ],
   "source": [
    "# Dataset: sliding windows\n",
    "context_length = 168   # 7 days of history\n",
    "prediction_length = 24  # 24h ahead\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, ctx_len, pred_len, start_idx, end_idx):\n",
    "        self.data = data\n",
    "        self.ctx_len = ctx_len\n",
    "        self.pred_len = pred_len\n",
    "        self.start = start_idx\n",
    "        self.end = end_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.end - self.start - self.ctx_len - self.pred_len + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.start + idx\n",
    "        x = self.data[i : i + self.ctx_len]\n",
    "        y = self.data[i + self.ctx_len : i + self.ctx_len + self.pred_len, 0]\n",
    "        x_future = self.data[i + self.ctx_len : i + self.ctx_len + self.pred_len, 1:]\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(x_future),\n",
    "            torch.from_numpy(y),\n",
    "        )\n",
    "\n",
    "train_end = int(train_mask.sum())\n",
    "train_ds = TimeSeriesDataset(all_data, context_length, prediction_length, 0, train_end)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Context: {context_length}h, Prediction: {prediction_length}h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:43:58.186774Z",
     "iopub.status.busy": "2026-02-16T16:43:58.186716Z",
     "iopub.status.idle": "2026-02-16T16:43:58.201229Z",
     "shell.execute_reply": "2026-02-16T16:43:58.201012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 78,978\n",
      "On device: mps\n"
     ]
    }
   ],
   "source": [
    "# DeepAR-style model: LSTM encoder + Gaussian output (mu, sigma)\n",
    "# Scheduled sampling: during training, randomly use model's own predictions\n",
    "# instead of ground truth to reduce train/test mismatch\n",
    "class ProbabilisticLSTM(nn.Module):\n",
    "    def __init__(self, input_size, future_size, hidden_size, num_layers, pred_len):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.1)\n",
    "        self.decoder_cell = nn.LSTMCell(1 + future_size, hidden_size)\n",
    "        self.mu_head = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_head = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x_context, x_future, y_target=None, tf_ratio=1.0):\n",
    "        _, (h, c) = self.encoder(x_context)\n",
    "        h, c = h[-1], c[-1]\n",
    "        mus, log_sigmas = [], []\n",
    "        prev_y = x_context[:, -1, 0:1]\n",
    "        \n",
    "        for t in range(self.pred_len):\n",
    "            dec_input = torch.cat([prev_y, x_future[:, t, :]], dim=1)\n",
    "            h, c = self.decoder_cell(dec_input, (h, c))\n",
    "            mu = self.mu_head(h)\n",
    "            log_sigma = self.sigma_head(h)\n",
    "            mus.append(mu)\n",
    "            log_sigmas.append(log_sigma)\n",
    "            \n",
    "            if y_target is not None and torch.rand(1).item() < tf_ratio:\n",
    "                prev_y = y_target[:, t:t+1]\n",
    "            else:\n",
    "                prev_y = mu.detach()\n",
    "        \n",
    "        return torch.cat(mus, dim=1), torch.cat(log_sigmas, dim=1)\n",
    "\n",
    "input_size = all_data.shape[1]\n",
    "future_size = len(feature_cols)\n",
    "\n",
    "model = ProbabilisticLSTM(\n",
    "    input_size=input_size,\n",
    "    future_size=future_size,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    pred_len=prediction_length,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"On device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:43:58.202165Z",
     "iopub.status.busy": "2026-02-16T16:43:58.202097Z",
     "iopub.status.idle": "2026-02-16T16:46:32.305470Z",
     "shell.execute_reply": "2026-02-16T16:46:32.303706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: -0.5485, TF: 1.00, LR: 0.00100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Loss: -1.2841, TF: 0.77, LR: 0.00090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Loss: -1.0603, TF: 0.49, LR: 0.00065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Loss: -0.9143, TF: 0.30, LR: 0.00035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Loss: -0.9732, TF: 0.30, LR: 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Loss: -1.0061, TF: 0.30, LR: 0.00000\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Train with Gaussian NLL loss + scheduled sampling\n",
    "# Teacher forcing ratio decays from 1.0 -> 0.3 over training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25)\n",
    "\n",
    "def gaussian_nll(mu, log_sigma, target):\n",
    "    sigma = torch.exp(log_sigma) + 1e-6\n",
    "    return torch.mean(0.5 * torch.log(sigma**2) + 0.5 * ((target - mu) / sigma) ** 2)\n",
    "\n",
    "n_epochs = 25\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    tf_ratio = max(0.3, 1.0 - epoch / (n_epochs * 0.7))\n",
    "    losses = []\n",
    "    for x_ctx, x_fut, y in train_loader:\n",
    "        x_ctx, x_fut, y = x_ctx.to(device), x_fut.to(device), y.to(device)\n",
    "        mu, log_sigma = model(x_ctx, x_fut, y, tf_ratio=tf_ratio)\n",
    "        loss = gaussian_nll(mu, log_sigma, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_loss = np.mean(losses)\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}, TF: {tf_ratio:.2f}, LR: {scheduler.get_last_lr()[0]:.5f}\")\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:46:32.312008Z",
     "iopub.status.busy": "2026-02-16T16:46:32.310679Z",
     "iopub.status.idle": "2026-02-16T16:46:36.928501Z",
     "shell.execute_reply": "2026-02-16T16:46:36.928247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 364 forecast windows across 2018\n"
     ]
    }
   ],
   "source": [
    "# Generate forecasts on 2018 test data\n",
    "model.eval()\n",
    "test_start = train_end\n",
    "test_end = len(all_data)\n",
    "\n",
    "all_mu = []\n",
    "all_sigma = []\n",
    "all_actuals_list = []\n",
    "all_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_start, test_end - prediction_length, prediction_length):\n",
    "        if i - context_length < 0:\n",
    "            continue\n",
    "        \n",
    "        x_ctx = torch.from_numpy(all_data[i - context_length : i]).unsqueeze(0).to(device)\n",
    "        x_fut = torch.from_numpy(all_data[i : i + prediction_length, 1:]).unsqueeze(0).to(device)\n",
    "        \n",
    "        mu, log_sigma = model(x_ctx, x_fut, tf_ratio=0.0)\n",
    "        sigma = torch.exp(log_sigma) + 1e-6\n",
    "        \n",
    "        # Denormalize\n",
    "        mu_mw = mu.squeeze().cpu().numpy() * target_std + target_mean\n",
    "        sigma_mw = sigma.squeeze().cpu().numpy() * target_std\n",
    "        actual_mw = all_data[i : i + prediction_length, 0] * target_std + target_mean\n",
    "        \n",
    "        times = df['time'].iloc[i : i + prediction_length].values\n",
    "        \n",
    "        all_mu.append(mu_mw)\n",
    "        all_sigma.append(sigma_mw)\n",
    "        all_actuals_list.append(actual_mw)\n",
    "        all_times.append(times)\n",
    "\n",
    "print(f\"Generated {len(all_mu)} forecast windows across 2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:46:36.929633Z",
     "iopub.status.busy": "2026-02-16T16:46:36.929567Z",
     "iopub.status.idle": "2026-02-16T16:46:55.943122Z",
     "shell.execute_reply": "2026-02-16T16:46:55.942814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction helps! Raw: 3391 -> Corrected: 3268 MW (3.6%)\n",
      "Final MAE:      3268 MW\n",
      "80% coverage:   79.7%\n",
      "90% coverage:   89.6%\n",
      "(No TSO forecast available for residual load)\n"
     ]
    }
   ],
   "source": [
    "# Residual correction: train on training data errors to fix systematic biases\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Run DeepAR inference on training data (no teacher forcing)\n",
    "train_mu_list, train_actual_list, train_feat_list = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i in range(context_length, train_end - prediction_length, prediction_length):\n",
    "        x_ctx = torch.from_numpy(all_data[i - context_length : i]).unsqueeze(0).to(device)\n",
    "        x_fut = torch.from_numpy(all_data[i : i + prediction_length, 1:]).unsqueeze(0).to(device)\n",
    "        mu, _ = model(x_ctx, x_fut, tf_ratio=0.0)\n",
    "        train_mu_list.append(mu.squeeze().cpu().numpy() * target_std + target_mean)\n",
    "        train_actual_list.append(all_data[i : i + prediction_length, 0] * target_std + target_mean)\n",
    "        train_feat_list.append(all_data[i : i + prediction_length, 1:])\n",
    "\n",
    "# Train per-horizon GBR correction models\n",
    "correction_models = []\n",
    "for h in range(prediction_length):\n",
    "    X_h = np.array([np.append(f[h], m[h]) for f, m in zip(train_feat_list, train_mu_list)])\n",
    "    y_h = np.array([a[h] - m[h] for a, m in zip(train_actual_list, train_mu_list)])\n",
    "    gbr = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    gbr.fit(X_h, y_h)\n",
    "    correction_models.append(gbr)\n",
    "\n",
    "# Apply correction to test predictions\n",
    "corrected_mu = []\n",
    "for i, mu_mw in enumerate(all_mu):\n",
    "    idx_start = test_start + i * prediction_length\n",
    "    feats = all_data[idx_start : idx_start + prediction_length, 1:]\n",
    "    corrected = np.array([mu_mw[h] + correction_models[h].predict(np.append(feats[h], mu_mw[h]).reshape(1, -1))[0]\n",
    "                          for h in range(prediction_length)])\n",
    "    corrected_mu.append(corrected)\n",
    "\n",
    "raw_mae = np.mean([np.mean(np.abs(a - m)) for a, m in zip(all_actuals_list, all_mu)])\n",
    "corr_mae = np.mean([np.mean(np.abs(a - c)) for a, c in zip(all_actuals_list, corrected_mu)])\n",
    "\n",
    "# Use correction only if it actually improves MAE\n",
    "if corr_mae < raw_mae:\n",
    "    final_mu = corrected_mu\n",
    "    final_mae = corr_mae\n",
    "    print(f\"Correction helps! Raw: {raw_mae:.0f} -> Corrected: {corr_mae:.0f} MW ({(1-corr_mae/raw_mae)*100:.1f}%)\")\n",
    "else:\n",
    "    final_mu = list(all_mu)\n",
    "    final_mae = raw_mae\n",
    "    print(f\"Correction doesn't help (Raw: {raw_mae:.0f}, Corrected: {corr_mae:.0f}). Using raw predictions.\")\n",
    "\n",
    "# Multi-quantile calibration (gradient fan chart)\n",
    "quantile_levels = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95]\n",
    "errors_by_horizon = [[] for _ in range(prediction_length)]\n",
    "for pred, actual in zip(final_mu, all_actuals_list):\n",
    "    for h in range(prediction_length):\n",
    "        errors_by_horizon[h].append(actual[h] - pred[h])\n",
    "\n",
    "quantile_offsets = {q: np.array([np.percentile(errors_by_horizon[h], q) for h in range(prediction_length)]) for q in quantile_levels}\n",
    "\n",
    "all_forecasts = []\n",
    "for pred in final_mu:\n",
    "    fc = {'p50': pred}\n",
    "    for q in quantile_levels:\n",
    "        fc[f'p{q}'] = pred + quantile_offsets[q]\n",
    "    all_forecasts.append(fc)\n",
    "\n",
    "cov_80 = np.mean([np.mean((a >= fc['p10']) & (a <= fc['p90'])) for a, fc in zip(all_actuals_list, all_forecasts)])\n",
    "cov_90 = np.mean([np.mean((a >= fc['p5']) & (a <= fc['p95'])) for a, fc in zip(all_actuals_list, all_forecasts)])\n",
    "\n",
    "# No TSO comparison for residual load (no TSO forecast exists)\n",
    "print(f\"Final MAE:      {final_mae:.0f} MW\")\n",
    "print(f\"80% coverage:   {cov_80*100:.1f}%\")\n",
    "print(f\"90% coverage:   {cov_90*100:.1f}%\")\n",
    "print(\"(No TSO forecast available for residual load)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:46:55.944286Z",
     "iopub.status.busy": "2026-02-16T16:46:55.944165Z",
     "iopub.status.idle": "2026-02-16T16:46:55.947807Z",
     "shell.execute_reply": "2026-02-16T16:46:55.947607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: 168 hours\n",
      "Quantiles exported: ['p5', 'p10', 'p20', 'p30', 'p40', 'p50', 'p60', 'p70', 'p80', 'p90', 'p95']\n"
     ]
    }
   ],
   "source": [
    "# Export sample week with multi-quantile data for gradient fan chart\n",
    "sample_windows = range(9, 16)\n",
    "\n",
    "sample_data = []\n",
    "for w in sample_windows:\n",
    "    if w >= len(all_forecasts):\n",
    "        break\n",
    "    fc = all_forecasts[w]\n",
    "    actual = all_actuals_list[w]\n",
    "    times = all_times[w]\n",
    "    \n",
    "    for h in range(prediction_length):\n",
    "        t = pd.Timestamp(times[h])\n",
    "        point = {\n",
    "            'time': t.strftime('%Y-%m-%d %H:%M'),\n",
    "            'actual': round(float(actual[h]), 1),\n",
    "        }\n",
    "        for q in quantile_levels:\n",
    "            point[f'p{q}'] = round(float(max(0, fc[f'p{q}'][h])), 1)\n",
    "        sample_data.append(point)\n",
    "\n",
    "print(f\"Sample data: {len(sample_data)} hours\")\n",
    "print(f\"Quantiles exported: {[f'p{q}' for q in quantile_levels]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:46:55.948743Z",
     "iopub.status.busy": "2026-02-16T16:46:55.948678Z",
     "iopub.status.idle": "2026-02-16T16:46:55.953501Z",
     "shell.execute_reply": "2026-02-16T16:46:55.953289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved deepar_residual.json\n",
      "MAE:          3268.1 MW\n",
      "80% Coverage: 79.7%\n",
      "90% Coverage: 89.6%\n"
     ]
    }
   ],
   "source": [
    "# Export JSON\n",
    "import os\n",
    "os.makedirs('../dashboard/public/data', exist_ok=True)\n",
    "\n",
    "output = {\n",
    "    'target': 'residual_load',\n",
    "    'model': 'DeepAR + GBR Correction' if corr_mae < raw_mae else 'DeepAR (LSTM + Gaussian)',\n",
    "    'prediction_length_hours': prediction_length,\n",
    "    'context_length_hours': context_length,\n",
    "    'metrics': {\n",
    "        'mae': round(final_mae, 1),\n",
    "        'coverage_80': round(cov_80 * 100, 1),\n",
    "        'coverage_90': round(cov_90 * 100, 1),\n",
    "    },\n",
    "    'quantile_levels': quantile_levels,\n",
    "    'sample_forecast': sample_data,\n",
    "}\n",
    "\n",
    "with open('../dashboard/public/data/deepar_residual.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print('Saved deepar_residual.json')\n",
    "print(f\"MAE:          {output['metrics']['mae']} MW\")\n",
    "print(f\"80% Coverage: {output['metrics']['coverage_80']}%\")\n",
    "print(f\"90% Coverage: {output['metrics']['coverage_90']}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
