{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 6 — PatchTST: Solar Generation & Electricity Price\n",
    "Multivariate Patch Time Series Transformer. Context (168h) split into 7 patches of 24h,\n",
    "future weather covariates as 8th token. Transformer encoder → 24h predictions.\n",
    "\n",
    "XGBoost residual correction per horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (35056, 80)\n",
      "Train: 26280, Test: 8776\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_parquet('../cleaned_data.parquet')\n",
    "df['time'] = pd.to_datetime(df['time'], utc=True)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "train_mask = df['time'].dt.year <= 2017\n",
    "train_end = int(train_mask.sum())\n",
    "test_start = train_end\n",
    "context_length = 168\n",
    "prediction_length = 24\n",
    "\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Train: {train_end}, Test: {len(df) - train_end}')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input channels: 23 (1 target + 22 features)\n",
      "Target mean: 1453 MW, std: 1687 MW\n"
     ]
    }
   ],
   "source": [
    "# Solar features: clouds, temp, temp_max, humidity × 5 cities + hour, month\n",
    "target_col = 'generation solar'\n",
    "tso_col = 'forecast solar day ahead'\n",
    "\n",
    "weather_cols = [\n",
    "    'clouds_all_madrid', 'clouds_all_bilbao', 'clouds_all_barcelona',\n",
    "    'clouds_all_seville', 'clouds_all_valencia',\n",
    "    'temp_madrid', 'temp_bilbao', 'temp_barcelona',\n",
    "    'temp_seville', 'temp_valencia',\n",
    "    'temp_max_madrid', 'temp_max_bilbao', 'temp_max_barcelona',\n",
    "    'temp_max_seville', 'temp_max_valencia',\n",
    "    'humidity_madrid', 'humidity_bilbao', 'humidity_barcelona',\n",
    "    'humidity_seville', 'humidity_valencia',\n",
    "]\n",
    "time_cols = ['hour', 'month']\n",
    "feature_cols = weather_cols + time_cols\n",
    "\n",
    "target_mean = df.loc[train_mask, target_col].mean()\n",
    "target_std = df.loc[train_mask, target_col].std()\n",
    "feat_means = df.loc[train_mask, feature_cols].mean()\n",
    "feat_stds = df.loc[train_mask, feature_cols].std().replace(0, 1)\n",
    "\n",
    "target_norm = (df[target_col].values - target_mean) / target_std\n",
    "features_norm = ((df[feature_cols] - feat_means) / feat_stds).fillna(0).values\n",
    "all_data = np.column_stack([target_norm, features_norm]).astype(np.float32)\n",
    "\n",
    "print(f'Input channels: {all_data.shape[1]} (1 target + {len(feature_cols)} features)')\n",
    "print(f'Target mean: {target_mean:.0f} MW, std: {target_std:.0f} MW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20846, Val: 5069\n"
     ]
    }
   ],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, ctx_len, pred_len, start_idx, end_idx):\n",
    "        self.data = data\n",
    "        self.ctx_len = ctx_len\n",
    "        self.pred_len = pred_len\n",
    "        self.start = start_idx\n",
    "        self.end = end_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end - self.start - self.ctx_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.start + idx\n",
    "        x = self.data[i : i + self.ctx_len]\n",
    "        y = self.data[i + self.ctx_len : i + self.ctx_len + self.pred_len, 0]\n",
    "        x_future = self.data[i + self.ctx_len : i + self.ctx_len + self.pred_len, 1:]\n",
    "        return torch.from_numpy(x), torch.from_numpy(x_future), torch.from_numpy(y)\n",
    "\n",
    "val_split = int(train_end * 0.8)\n",
    "train_ds = TimeSeriesDataset(all_data, context_length, prediction_length, 0, val_split)\n",
    "val_ds = TimeSeriesDataset(all_data, context_length, prediction_length, val_split, train_end)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTST parameters: 205,592\n"
     ]
    }
   ],
   "source": [
    "class PatchTST(nn.Module):\n",
    "    def __init__(self, n_input, n_future, d_model=64, n_heads=4, n_layers=2,\n",
    "                 pred_len=24, ctx_len=168, patch_len=24, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_patches = ctx_len // patch_len  # 7\n",
    "        self.patch_embed = nn.Linear(patch_len * n_input, d_model)\n",
    "        self.future_embed = nn.Linear(pred_len * n_future, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, d_model) * 0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model * 2,\n",
    "            dropout=dropout, batch_first=True, norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        total_dim = (self.n_patches + 1) * d_model\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(total_dim, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, pred_len),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_context, x_future):\n",
    "        bs = x_context.shape[0]\n",
    "        patches = x_context.reshape(bs, self.n_patches, -1)\n",
    "        patch_tokens = self.patch_embed(patches)\n",
    "        future_token = self.future_embed(x_future.reshape(bs, -1)).unsqueeze(1)\n",
    "        tokens = torch.cat([patch_tokens, future_token], dim=1) + self.pos_embed\n",
    "        out = self.norm(self.transformer(tokens))\n",
    "        return self.head(out.reshape(bs, -1))\n",
    "\n",
    "n_input = all_data.shape[1]\n",
    "n_future = len(feature_cols)\n",
    "model = PatchTST(n_input, n_future).to(device)\n",
    "print(f'PatchTST parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_patchtst(model, train_loader, val_loader, n_epochs=60, patience=10, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for x, xf, y in train_loader:\n",
    "            x, xf, y = x.to(device), xf.to(device), y.to(device)\n",
    "            loss = criterion(model(x, xf), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, xf, y in val_loader:\n",
    "                x, xf, y = x.to(device), xf.to(device), y.to(device)\n",
    "                val_losses.append(criterion(model(x, xf), y).item())\n",
    "\n",
    "        scheduler.step()\n",
    "        tl, vl = np.mean(train_losses), np.mean(val_losses)\n",
    "\n",
    "        if vl < best_val:\n",
    "            best_val = vl\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "            mark = ' *'\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            mark = ''\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or mark:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs}, Train: {tl:.5f}, Val: {vl:.5f}{mark}')\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f'Best val loss: {best_val:.5f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training PatchTST for Solar ===\n",
      "Epoch   1/60, Train: 0.20665, Val: 0.16789 *\n",
      "Epoch   2/60, Train: 0.13268, Val: 0.16303 *\n",
      "Early stopping at epoch 12\n",
      "Best val loss: 0.16303\n",
      "\n",
      "Test windows: 364\n",
      "Raw PatchTST Solar MAE: 469.3 MW\n",
      "Train prediction windows: 1088\n"
     ]
    }
   ],
   "source": [
    "print('=== Training PatchTST for Solar ===')\n",
    "model = train_patchtst(model, train_loader, val_loader)\n",
    "\n",
    "# Test predictions\n",
    "model.eval()\n",
    "test_preds, test_actuals, test_times = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_start, len(all_data) - prediction_length, prediction_length):\n",
    "        if i - context_length < 0:\n",
    "            continue\n",
    "        x = torch.from_numpy(all_data[i - context_length : i]).unsqueeze(0).to(device)\n",
    "        xf = torch.from_numpy(all_data[i : i + prediction_length, 1:]).unsqueeze(0).to(device)\n",
    "        pred = model(x, xf).squeeze().cpu().numpy() * target_std + target_mean\n",
    "        pred = np.clip(pred, 0, None)\n",
    "        actual = all_data[i : i + prediction_length, 0] * target_std + target_mean\n",
    "        test_preds.append(pred)\n",
    "        test_actuals.append(actual)\n",
    "        test_times.append(df['time'].iloc[i : i + prediction_length].values)\n",
    "\n",
    "raw_mae = np.mean([np.mean(np.abs(a - p)) for a, p in zip(test_actuals, test_preds)])\n",
    "print(f'\\nTest windows: {len(test_preds)}')\n",
    "print(f'Raw PatchTST Solar MAE: {raw_mae:.1f} MW')\n",
    "\n",
    "# Train predictions (for stacking notebook)\n",
    "train_preds_raw, train_windows_info = [], []\n",
    "with torch.no_grad():\n",
    "    w = 0\n",
    "    for i in range(context_length, train_end - prediction_length, prediction_length):\n",
    "        x = torch.from_numpy(all_data[i - context_length : i]).unsqueeze(0).to(device)\n",
    "        xf = torch.from_numpy(all_data[i : i + prediction_length, 1:]).unsqueeze(0).to(device)\n",
    "        pred = model(x, xf).squeeze().cpu().numpy() * target_std + target_mean\n",
    "        pred = np.clip(pred, 0, None)\n",
    "        train_preds_raw.append(pred)\n",
    "        train_windows_info.append((w, i))\n",
    "        w += 1\n",
    "\n",
    "print(f'Train prediction windows: {len(train_preds_raw)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost correction: 870 train, 218 val windows\n",
      "Trees per horizon: min=71, max=300, mean=187\n",
      "\n",
      "Correction helps! Raw: 469.3 -> Corrected: 366.6 MW\n",
      "TSO MAE: 141.1 MW\n"
     ]
    }
   ],
   "source": [
    "# XGBoost residual correction for solar\n",
    "xgb_feats, xgb_actuals, xgb_tso = [], [], []\n",
    "for w_idx, (w, start_i) in enumerate(train_windows_info):\n",
    "    xgb_feats.append(all_data[start_i : start_i + prediction_length, 1:])\n",
    "    xgb_actuals.append(all_data[start_i : start_i + prediction_length, 0] * target_std + target_mean)\n",
    "    xgb_tso.append(df[tso_col].iloc[start_i : start_i + prediction_length].values)\n",
    "\n",
    "n_w = len(train_preds_raw)\n",
    "xgb_split = int(n_w * 0.8)\n",
    "print(f'XGBoost correction: {xgb_split} train, {n_w - xgb_split} val windows')\n",
    "\n",
    "correction_models = []\n",
    "for h in range(prediction_length):\n",
    "    X = np.array([np.concatenate([f[h], [p[h], t[h]]])\n",
    "                  for f, p, t in zip(xgb_feats, train_preds_raw, xgb_tso)])\n",
    "    y = np.array([a[h] - p[h] for a, p in zip(xgb_actuals, train_preds_raw)])\n",
    "    X_tr, X_va = X[:xgb_split], X[xgb_split:]\n",
    "    y_tr, y_va = y[:xgb_split], y[xgb_split:]\n",
    "    xgbr = xgb.XGBRegressor(\n",
    "        n_estimators=500, max_depth=4, learning_rate=0.05,\n",
    "        tree_method='hist', random_state=42, verbosity=0, early_stopping_rounds=50,\n",
    "    )\n",
    "    xgbr.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "    correction_models.append(xgbr)\n",
    "\n",
    "stopped = [m.best_iteration + 1 for m in correction_models]\n",
    "print(f'Trees per horizon: min={min(stopped)}, max={max(stopped)}, mean={np.mean(stopped):.0f}')\n",
    "\n",
    "# Apply to test\n",
    "corrected_preds = []\n",
    "for i, pred in enumerate(test_preds):\n",
    "    idx = test_start + i * prediction_length\n",
    "    feats = all_data[idx : idx + prediction_length, 1:]\n",
    "    tso = df[tso_col].iloc[idx : idx + prediction_length].values\n",
    "    corrected = np.array([\n",
    "        pred[h] + correction_models[h].predict(\n",
    "            np.concatenate([feats[h], [pred[h], tso[h]]]).reshape(1, -1)\n",
    "        )[0] for h in range(prediction_length)\n",
    "    ])\n",
    "    corrected_preds.append(np.clip(corrected, 0, None))\n",
    "\n",
    "corr_mae = np.mean([np.mean(np.abs(a - c)) for a, c in zip(test_actuals, corrected_preds)])\n",
    "flat_actual = np.concatenate(test_actuals)\n",
    "flat_corr = np.concatenate(corrected_preds)\n",
    "flat_raw = np.concatenate(test_preds)\n",
    "corr_rmse = np.sqrt(np.mean((flat_actual - flat_corr) ** 2))\n",
    "corr_mape = np.mean(np.abs((flat_actual - flat_corr) / np.clip(np.abs(flat_actual), 1, None))) * 100\n",
    "\n",
    "tso_flat = np.concatenate([df[tso_col].iloc[test_start + w * prediction_length :\n",
    "    test_start + w * prediction_length + prediction_length].values for w in range(len(test_preds))])\n",
    "tso_mae = np.mean(np.abs(flat_actual - tso_flat))\n",
    "tso_rmse = np.sqrt(np.mean((flat_actual - tso_flat) ** 2))\n",
    "\n",
    "if corr_mae < raw_mae:\n",
    "    final_preds_s = corrected_preds\n",
    "    final_mae_s = corr_mae\n",
    "    print(f'\\nCorrection helps! Raw: {raw_mae:.1f} -> Corrected: {corr_mae:.1f} MW')\n",
    "else:\n",
    "    final_preds_s = test_preds\n",
    "    final_mae_s = raw_mae\n",
    "    corr_rmse = np.sqrt(np.mean((flat_actual - flat_raw) ** 2))\n",
    "    corr_mape = np.mean(np.abs((flat_actual - flat_raw) / np.clip(np.abs(flat_actual), 1, None))) * 100\n",
    "    print(f'\\nCorrection does not help. Using raw: {raw_mae:.1f} MW')\n",
    "\n",
    "print(f'TSO MAE: {tso_mae:.1f} MW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved patchtst_solar.json (MAE: 366.6 MW)\n",
      "Saved patchtst_solar_predictions.csv (8736 rows)\n",
      "Saved patchtst_solar_train_predictions.csv (26112 rows)\n"
     ]
    }
   ],
   "source": [
    "# Export solar: JSON + CSVs\n",
    "os.makedirs('../dashboard/public/data', exist_ok=True)\n",
    "\n",
    "sample_data = []\n",
    "for w in range(9, 16):\n",
    "    if w >= len(final_preds_s):\n",
    "        break\n",
    "    for h in range(prediction_length):\n",
    "        idx = test_start + w * prediction_length + h\n",
    "        t = pd.Timestamp(test_times[w][h])\n",
    "        sample_data.append({\n",
    "            'time': t.strftime('%Y-%m-%d %H:%M'),\n",
    "            'actual': round(float(test_actuals[w][h]), 1),\n",
    "            'predicted': round(float(final_preds_s[w][h]), 1),\n",
    "            'tso': round(float(df[tso_col].iloc[idx]), 1),\n",
    "        })\n",
    "\n",
    "avg_trees = int(np.mean(stopped))\n",
    "output = {\n",
    "    'target': 'solar',\n",
    "    'model': f'PatchTST + XGBoost ({avg_trees} trees avg)',\n",
    "    'prediction_length_hours': prediction_length,\n",
    "    'context_length_hours': context_length,\n",
    "    'metrics': {\n",
    "        'mae': round(float(final_mae_s), 1),\n",
    "        'rmse': round(float(corr_rmse), 1),\n",
    "        'mape': round(float(corr_mape), 1),\n",
    "        'tso_mae': round(float(tso_mae), 1),\n",
    "        'tso_rmse': round(float(tso_rmse), 1),\n",
    "        'raw_mae': round(float(raw_mae), 1),\n",
    "        'raw_rmse': round(float(np.sqrt(np.mean((flat_actual - flat_raw) ** 2))), 1),\n",
    "    },\n",
    "    'sample_forecast': sample_data,\n",
    "}\n",
    "\n",
    "with open('../dashboard/public/data/patchtst_solar.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "# Test CSV\n",
    "rows = []\n",
    "for w, (pred, actual, times) in enumerate(zip(test_preds, test_actuals, test_times)):\n",
    "    idx = test_start + w * prediction_length\n",
    "    tso = df[tso_col].iloc[idx : idx + prediction_length].values\n",
    "    for h in range(prediction_length):\n",
    "        rows.append({\n",
    "            'time': pd.Timestamp(times[h]).strftime('%Y-%m-%d %H:%M'),\n",
    "            'window': w, 'horizon': h,\n",
    "            'actual': round(float(actual[h]), 2),\n",
    "            'patchtst_pred': round(float(pred[h]), 2),\n",
    "            'tso': round(float(tso[h]), 2),\n",
    "        })\n",
    "pd.DataFrame(rows).to_csv('patchtst_solar_predictions.csv', index=False)\n",
    "\n",
    "# Train CSV\n",
    "train_rows = []\n",
    "for pred, (w, start_i) in zip(train_preds_raw, train_windows_info):\n",
    "    for h in range(prediction_length):\n",
    "        train_rows.append({\n",
    "            'global_idx': start_i + h,\n",
    "            'window': w, 'horizon': h,\n",
    "            'patchtst_pred': round(float(pred[h]), 2),\n",
    "        })\n",
    "pd.DataFrame(train_rows).to_csv('patchtst_solar_train_predictions.csv', index=False)\n",
    "\n",
    "print(f'Saved patchtst_solar.json (MAE: {output[\"metrics\"][\"mae\"]} MW)')\n",
    "print(f'Saved patchtst_solar_predictions.csv ({len(rows)} rows)')\n",
    "print(f'Saved patchtst_solar_train_predictions.csv ({len(train_rows)} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Price Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price input: 33 (1 target + 32 features)\n",
      "Price PatchTST parameters: 236,312\n"
     ]
    }
   ],
   "source": [
    "# Price features: pressure, temp, temp_max, temp_min, humidity, wind_speed × 5 cities + hour, month\n",
    "target_col_p = 'price actual'\n",
    "tso_col_p = 'price day ahead'\n",
    "\n",
    "weather_cols_p = [\n",
    "    'pressure_madrid', 'pressure_bilbao', 'pressure_barcelona',\n",
    "    'pressure_seville', 'pressure_valencia',\n",
    "    'temp_madrid', 'temp_bilbao', 'temp_barcelona',\n",
    "    'temp_seville', 'temp_valencia',\n",
    "    'temp_max_madrid', 'temp_max_bilbao', 'temp_max_barcelona',\n",
    "    'temp_max_seville', 'temp_max_valencia',\n",
    "    'temp_min_madrid', 'temp_min_bilbao', 'temp_min_barcelona',\n",
    "    'temp_min_seville', 'temp_min_valencia',\n",
    "    'humidity_madrid', 'humidity_bilbao', 'humidity_barcelona',\n",
    "    'humidity_seville', 'humidity_valencia',\n",
    "    'wind_speed_madrid', 'wind_speed_bilbao', 'wind_speed_barcelona',\n",
    "    'wind_speed_seville', 'wind_speed_valencia',\n",
    "]\n",
    "time_cols_p = ['hour', 'month']\n",
    "feature_cols_p = weather_cols_p + time_cols_p\n",
    "\n",
    "target_mean_p = df.loc[train_mask, target_col_p].mean()\n",
    "target_std_p = df.loc[train_mask, target_col_p].std()\n",
    "feat_means_p = df.loc[train_mask, feature_cols_p].mean()\n",
    "feat_stds_p = df.loc[train_mask, feature_cols_p].std().replace(0, 1)\n",
    "\n",
    "target_norm_p = (df[target_col_p].values - target_mean_p) / target_std_p\n",
    "features_norm_p = ((df[feature_cols_p] - feat_means_p) / feat_stds_p).fillna(0).values\n",
    "all_data_p = np.column_stack([target_norm_p, features_norm_p]).astype(np.float32)\n",
    "\n",
    "# Free solar model/data to save memory\n",
    "del model, all_data, train_ds, val_ds, train_loader, val_loader\n",
    "del correction_models, xgb_feats, xgb_actuals, xgb_tso\n",
    "import gc; gc.collect()\n",
    "\n",
    "train_ds_p = TimeSeriesDataset(all_data_p, context_length, prediction_length, 0, val_split)\n",
    "val_ds_p = TimeSeriesDataset(all_data_p, context_length, prediction_length, val_split, train_end)\n",
    "train_loader_p = DataLoader(train_ds_p, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader_p = DataLoader(val_ds_p, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "n_input_p = all_data_p.shape[1]\n",
    "n_future_p = len(feature_cols_p)\n",
    "model_p = PatchTST(n_input_p, n_future_p).to(device)\n",
    "\n",
    "print(f'Price input: {all_data_p.shape[1]} (1 target + {len(feature_cols_p)} features)')\n",
    "print(f'Price PatchTST parameters: {sum(p.numel() for p in model_p.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training PatchTST for Price ===\n",
      "Epoch   1/60, Train: 0.23254, Val: 0.17584 *\n",
      "Epoch   2/60, Train: 0.11312, Val: 0.15701 *\n",
      "Epoch   3/60, Train: 0.08715, Val: 0.14961 *\n",
      "Epoch   6/60, Train: 0.06377, Val: 0.14886 *\n",
      "Early stopping at epoch 16\n",
      "Best val loss: 0.14886\n",
      "\n",
      "Test windows: 364\n",
      "Raw PatchTST Price MAE: 5.13 EUR/MWh\n",
      "Train prediction windows: 1088\n"
     ]
    }
   ],
   "source": [
    "print('=== Training PatchTST for Price ===')\n",
    "model_p = train_patchtst(model_p, train_loader_p, val_loader_p)\n",
    "\n",
    "# Test predictions\n",
    "model_p.eval()\n",
    "test_preds_p, test_actuals_p, test_times_p = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_start, len(all_data_p) - prediction_length, prediction_length):\n",
    "        if i - context_length < 0:\n",
    "            continue\n",
    "        x = torch.from_numpy(all_data_p[i - context_length : i]).unsqueeze(0).to(device)\n",
    "        xf = torch.from_numpy(all_data_p[i : i + prediction_length, 1:]).unsqueeze(0).to(device)\n",
    "        pred = model_p(x, xf).squeeze().cpu().numpy() * target_std_p + target_mean_p\n",
    "        actual = all_data_p[i : i + prediction_length, 0] * target_std_p + target_mean_p\n",
    "        test_preds_p.append(pred)\n",
    "        test_actuals_p.append(actual)\n",
    "        test_times_p.append(df['time'].iloc[i : i + prediction_length].values)\n",
    "\n",
    "raw_mae_p = np.mean([np.mean(np.abs(a - p)) for a, p in zip(test_actuals_p, test_preds_p)])\n",
    "print(f'\\nTest windows: {len(test_preds_p)}')\n",
    "print(f'Raw PatchTST Price MAE: {raw_mae_p:.2f} EUR/MWh')\n",
    "\n",
    "# Train predictions for stacking\n",
    "train_preds_raw_p, train_windows_info_p = [], []\n",
    "with torch.no_grad():\n",
    "    w = 0\n",
    "    for i in range(context_length, train_end - prediction_length, prediction_length):\n",
    "        x = torch.from_numpy(all_data_p[i - context_length : i]).unsqueeze(0).to(device)\n",
    "        xf = torch.from_numpy(all_data_p[i : i + prediction_length, 1:]).unsqueeze(0).to(device)\n",
    "        pred = model_p(x, xf).squeeze().cpu().numpy() * target_std_p + target_mean_p\n",
    "        train_preds_raw_p.append(pred)\n",
    "        train_windows_info_p.append((w, i))\n",
    "        w += 1\n",
    "\n",
    "print(f'Train prediction windows: {len(train_preds_raw_p)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost correction: 870 train, 218 val windows\n",
      "Trees per horizon: min=1, max=284, mean=87\n",
      "\n",
      "Correction helps! Raw: 5.13 -> Corrected: 5.06 EUR/MWh\n",
      "TSO MAE: 8.87 EUR/MWh\n"
     ]
    }
   ],
   "source": [
    "# XGBoost residual correction for price\n",
    "xgb_feats_p, xgb_actuals_p, xgb_tso_p = [], [], []\n",
    "for w_idx, (w, start_i) in enumerate(train_windows_info_p):\n",
    "    xgb_feats_p.append(all_data_p[start_i : start_i + prediction_length, 1:])\n",
    "    xgb_actuals_p.append(all_data_p[start_i : start_i + prediction_length, 0] * target_std_p + target_mean_p)\n",
    "    xgb_tso_p.append(df[tso_col_p].iloc[start_i : start_i + prediction_length].values)\n",
    "\n",
    "n_w_p = len(train_preds_raw_p)\n",
    "xgb_split_p = int(n_w_p * 0.8)\n",
    "print(f'XGBoost correction: {xgb_split_p} train, {n_w_p - xgb_split_p} val windows')\n",
    "\n",
    "correction_models_p = []\n",
    "for h in range(prediction_length):\n",
    "    X = np.array([np.concatenate([f[h], [p[h], t[h]]])\n",
    "                  for f, p, t in zip(xgb_feats_p, train_preds_raw_p, xgb_tso_p)])\n",
    "    y = np.array([a[h] - p[h] for a, p in zip(xgb_actuals_p, train_preds_raw_p)])\n",
    "    X_tr, X_va = X[:xgb_split_p], X[xgb_split_p:]\n",
    "    y_tr, y_va = y[:xgb_split_p], y[xgb_split_p:]\n",
    "    xgbr = xgb.XGBRegressor(\n",
    "        n_estimators=500, max_depth=4, learning_rate=0.05,\n",
    "        tree_method='hist', random_state=42, verbosity=0, early_stopping_rounds=50,\n",
    "    )\n",
    "    xgbr.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "    correction_models_p.append(xgbr)\n",
    "\n",
    "stopped_p = [m.best_iteration + 1 for m in correction_models_p]\n",
    "print(f'Trees per horizon: min={min(stopped_p)}, max={max(stopped_p)}, mean={np.mean(stopped_p):.0f}')\n",
    "\n",
    "# Apply to test\n",
    "corrected_preds_p = []\n",
    "for i, pred in enumerate(test_preds_p):\n",
    "    idx = test_start + i * prediction_length\n",
    "    feats = all_data_p[idx : idx + prediction_length, 1:]\n",
    "    tso = df[tso_col_p].iloc[idx : idx + prediction_length].values\n",
    "    corrected = np.array([\n",
    "        pred[h] + correction_models_p[h].predict(\n",
    "            np.concatenate([feats[h], [pred[h], tso[h]]]).reshape(1, -1)\n",
    "        )[0] for h in range(prediction_length)\n",
    "    ])\n",
    "    corrected_preds_p.append(corrected)\n",
    "\n",
    "corr_mae_p = np.mean([np.mean(np.abs(a - c)) for a, c in zip(test_actuals_p, corrected_preds_p)])\n",
    "flat_actual_p = np.concatenate(test_actuals_p)\n",
    "flat_corr_p = np.concatenate(corrected_preds_p)\n",
    "flat_raw_p = np.concatenate(test_preds_p)\n",
    "corr_rmse_p = np.sqrt(np.mean((flat_actual_p - flat_corr_p) ** 2))\n",
    "corr_mape_p = np.mean(np.abs((flat_actual_p - flat_corr_p) / np.clip(np.abs(flat_actual_p), 1, None))) * 100\n",
    "\n",
    "tso_flat_p = np.concatenate([df[tso_col_p].iloc[test_start + w * prediction_length :\n",
    "    test_start + w * prediction_length + prediction_length].values for w in range(len(test_preds_p))])\n",
    "tso_mae_p = np.mean(np.abs(flat_actual_p - tso_flat_p))\n",
    "tso_rmse_p = np.sqrt(np.mean((flat_actual_p - tso_flat_p) ** 2))\n",
    "\n",
    "if corr_mae_p < raw_mae_p:\n",
    "    final_preds_p = corrected_preds_p\n",
    "    final_mae_p = corr_mae_p\n",
    "    print(f'\\nCorrection helps! Raw: {raw_mae_p:.2f} -> Corrected: {corr_mae_p:.2f} EUR/MWh')\n",
    "else:\n",
    "    final_preds_p = test_preds_p\n",
    "    final_mae_p = raw_mae_p\n",
    "    corr_rmse_p = np.sqrt(np.mean((flat_actual_p - flat_raw_p) ** 2))\n",
    "    corr_mape_p = np.mean(np.abs((flat_actual_p - flat_raw_p) / np.clip(np.abs(flat_actual_p), 1, None))) * 100\n",
    "    print(f'\\nCorrection does not help. Using raw: {raw_mae_p:.2f} EUR/MWh')\n",
    "\n",
    "print(f'TSO MAE: {tso_mae_p:.2f} EUR/MWh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved patchtst_price.json (MAE: 5.06 EUR/MWh)\n",
      "Saved patchtst_price_predictions.csv (8736 rows)\n",
      "Saved patchtst_price_train_predictions.csv (26112 rows)\n"
     ]
    }
   ],
   "source": [
    "# Export price: JSON + CSVs\n",
    "sample_data_p = []\n",
    "for w in range(9, 16):\n",
    "    if w >= len(final_preds_p):\n",
    "        break\n",
    "    for h in range(prediction_length):\n",
    "        idx = test_start + w * prediction_length + h\n",
    "        t = pd.Timestamp(test_times_p[w][h])\n",
    "        sample_data_p.append({\n",
    "            'time': t.strftime('%Y-%m-%d %H:%M'),\n",
    "            'actual': round(float(test_actuals_p[w][h]), 2),\n",
    "            'predicted': round(float(final_preds_p[w][h]), 2),\n",
    "            'tso': round(float(df[tso_col_p].iloc[idx]), 2),\n",
    "        })\n",
    "\n",
    "avg_trees_p = int(np.mean(stopped_p))\n",
    "output_p = {\n",
    "    'target': 'price',\n",
    "    'model': f'PatchTST + XGBoost ({avg_trees_p} trees avg)',\n",
    "    'prediction_length_hours': prediction_length,\n",
    "    'context_length_hours': context_length,\n",
    "    'metrics': {\n",
    "        'mae': round(float(final_mae_p), 2),\n",
    "        'rmse': round(float(corr_rmse_p), 2),\n",
    "        'mape': round(float(corr_mape_p), 2),\n",
    "        'tso_mae': round(float(tso_mae_p), 2),\n",
    "        'tso_rmse': round(float(tso_rmse_p), 2),\n",
    "        'raw_mae': round(float(raw_mae_p), 2),\n",
    "        'raw_rmse': round(float(np.sqrt(np.mean((flat_actual_p - flat_raw_p) ** 2))), 2),\n",
    "    },\n",
    "    'sample_forecast': sample_data_p,\n",
    "}\n",
    "\n",
    "with open('../dashboard/public/data/patchtst_price.json', 'w') as f:\n",
    "    json.dump(output_p, f, indent=2)\n",
    "\n",
    "# Test CSV\n",
    "rows_p = []\n",
    "for w, (pred, actual, times) in enumerate(zip(test_preds_p, test_actuals_p, test_times_p)):\n",
    "    idx = test_start + w * prediction_length\n",
    "    tso = df[tso_col_p].iloc[idx : idx + prediction_length].values\n",
    "    for h in range(prediction_length):\n",
    "        rows_p.append({\n",
    "            'time': pd.Timestamp(times[h]).strftime('%Y-%m-%d %H:%M'),\n",
    "            'window': w, 'horizon': h,\n",
    "            'actual': round(float(actual[h]), 2),\n",
    "            'patchtst_pred': round(float(pred[h]), 2),\n",
    "            'tso': round(float(tso[h]), 2),\n",
    "        })\n",
    "pd.DataFrame(rows_p).to_csv('patchtst_price_predictions.csv', index=False)\n",
    "\n",
    "# Train CSV\n",
    "train_rows_p = []\n",
    "for pred, (w, start_i) in zip(train_preds_raw_p, train_windows_info_p):\n",
    "    for h in range(prediction_length):\n",
    "        train_rows_p.append({\n",
    "            'global_idx': start_i + h,\n",
    "            'window': w, 'horizon': h,\n",
    "            'patchtst_pred': round(float(pred[h]), 2),\n",
    "        })\n",
    "pd.DataFrame(train_rows_p).to_csv('patchtst_price_train_predictions.csv', index=False)\n",
    "\n",
    "print(f'Saved patchtst_price.json (MAE: {output_p[\"metrics\"][\"mae\"]} EUR/MWh)')\n",
    "print(f'Saved patchtst_price_predictions.csv ({len(rows_p)} rows)')\n",
    "print(f'Saved patchtst_price_train_predictions.csv ({len(train_rows_p)} rows)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
